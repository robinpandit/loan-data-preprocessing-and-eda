{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85300d266ca673fec60d4d093668e4dff1a09861"
   },
   "source": [
    "# Loan Approval Prediction: \n",
    "### EDA + Decision Tree, Random Forest & Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1976f202fdc13b5c36a5a615fe5d555ae73f93d7"
   },
   "source": [
    "Hello !\n",
    "I am Anoop and  in this kernel we will explore loan approval dataset from Analytics Vidya competition.\n",
    "<br>Please __Upvote__ this kernel if you like content.\n",
    "#Problem Statement:\n",
    "\n",
    "__About Company__ <br>\n",
    "Dream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n",
    "\n",
    "__Problem__ <br>\n",
    "Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n",
    "\n",
    "#### Dataset Description:\n",
    "\n",
    "| Variable | Description | \n",
    "|------|------|\n",
    "| Loan_ID | Unique Loan ID | \n",
    "| Gender | Male/ Female | \n",
    "| Married | Applicant married (Y/N) | \n",
    "| Dependents | Number of dependents | \n",
    "| Education | Applicant Education (Graduate/ Under Graduate) | \n",
    "| Self_Employed | Self employed (Y/N) | \n",
    "| ApplicantIncome | Applicant income | \n",
    "| CoapplicantIncome | Coapplicant income | \n",
    "| LoanAmount | Loan amount in thousands | \n",
    "| Loan_Amount_Term | Term of loan in months | \n",
    "| Credit_History | credit history meets guidelines | \n",
    "| Property_Area | Urban/ Semi Urban/ Rural | \n",
    "| Loan_Status | Loan approved (Y/N) | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "2e636af60c64ce6e4ab9eb2e673c431b2428d8bc"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83ee9356fad4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/train.csv' does not exist: b'../input/train.csv'"
     ]
    }
   ],
   "source": [
    "################### Importing Libraries ######################\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a4697b281e0f2fd86b17b298186ecceddb5a0fe"
   },
   "source": [
    "##### Observations:\n",
    "1. We can see there are total 13 columns including target variable, all of them are self explanatory. \n",
    "2. We also see some missing values, lets take stock of missing columns and what are the possible values for categorical and numerical columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52bb8de43b1cad0bb0036d7847c7bde259927692"
   },
   "outputs": [],
   "source": [
    "############ Count number of Categorical and Numerical Columns ######################\n",
    "train_df = train_df.drop(columns=['Loan_ID']) ## Dropping Loan ID\n",
    "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n",
    "#categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Loan_Amount_Term']\n",
    "\n",
    "print(categorical_columns)\n",
    "numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\n",
    "print(numerical_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23045afec9e84dcfd7abcbe0cc82b0e14a149f7b"
   },
   "source": [
    "#### Analyze values assigned to columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb1e277cafbb1d8469df6fed6a7c5c89a6d32d58"
   },
   "outputs": [],
   "source": [
    "### Data Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(4,2,figsize=(12,15))\n",
    "for idx,cat_col in enumerate(categorical_columns):\n",
    "    row,col = idx//2,idx%2\n",
    "    sns.countplot(x=cat_col,data=train_df,hue='Loan_Status',ax=axes[row,col])\n",
    "\n",
    "\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d762772e396fb9e418c4681e67577ce66ab4a62f"
   },
   "source": [
    "#### Plots above convey following things about the dataset:\n",
    "1. Loan Approval Status: About 2/3rd of applicants have been granted loan.\n",
    "2. Sex: There are more Men  than Women (approx. 3x) \n",
    "3. Martial Status: 2/3rd of the population in the dataset is Marred; Married applicants are more likely to be granted loans.\n",
    "4. Dependents: Majority of the population have zero dependents and are also likely to accepted for loan.\n",
    "5. Education: About 5/6th of the population is Graduate and graduates have higher propotion of loan approval\n",
    "6. Employment: 5/6th of population is not self employed.\n",
    "7. Property Area: More applicants from Semi-urban and also likely to be granted loans.\n",
    "8. Applicant with credit history are far more likely to be accepted.\n",
    "9. Loan Amount Term: Majority of the loans taken are for 360 Months (30 years).\n",
    "\n",
    "Now, let's also analyze Numerical Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "605f3e7876f51748af7c2c5588b7592691151667"
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(17,5))\n",
    "for idx,cat_col in enumerate(numerical_columns):\n",
    "    sns.boxplot(y=cat_col,data=train_df,x='Loan_Status',ax=axes[idx])\n",
    "\n",
    "print(train_df[numerical_columns].describe())\n",
    "plt.subplots_adjust(hspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b6a6d6b728423a1da70b25d8943eec34a9771ac"
   },
   "source": [
    "For Numercical Columns, there is no significant relation to Loan approval status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f9e340ec6989242f4276872955ae5fd44a136c55"
   },
   "source": [
    "### Preprocessing Data:\n",
    "Input data needs to be pre-processed before we feed it to model. Following things need to be taken care:\n",
    "1. Encoding Categorical Features.\n",
    "2. Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "37ba5a08c0113aa836127ea0199f40315e5ecd9f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0c8bed61db6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#### Encoding categrical Features: ##########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_df_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtrain_df_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "#### Encoding categrical Features: ##########\n",
    "train_df_encoded = pd.get_dummies(train_df,drop_first=True)\n",
    "train_df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c2fb4b1de74e27e787477900233b3122fcdbae6b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-17e28e1414a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m########## Split Features and Target Varible ############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df_encoded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Loan_Status_Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Loan_Status_Y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m################# Splitting into Train -Test Data #######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "########## Split Features and Target Varible ############\n",
    "X = train_df_encoded.drop(columns='Loan_Status_Y')\n",
    "y = train_df_encoded['Loan_Status_Y']\n",
    "\n",
    "################# Splitting into Train -Test Data #######\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify =y,random_state =42)\n",
    "############### Handling/Imputing Missing values #############\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "imp_train = imp.fit(X_train)\n",
    "X_train = imp_train.transform(X_train)\n",
    "X_test_imp = imp_train.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d12310c8bae698527bd399ecad2e5bbf3af36a0"
   },
   "source": [
    "### Model 1: Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "652d6255869bb59a347610940549fe7688f23225"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-08a1a26c2d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtree_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtree_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training Data Set Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train,y_train)\n",
    "y_pred = tree_clf.predict(X_train)\n",
    "print(\"Training Data Set Accuracy: \", accuracy_score(y_train,y_pred))\n",
    "print(\"Training Data F1 Score \", f1_score(y_train,y_pred))\n",
    "\n",
    "print(\"Validation Mean F1 Score: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\n",
    "print(\"Validation Mean Accuracy: \",cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b1e20932d504ecfdd5d6c9342039a67ad5c63cb"
   },
   "source": [
    "#### Overfitting Problem\n",
    "We can see from above metrics that Training Accuracy > Test Accuracy with default settings of Decision Tree classifier. Hence, model is overfit. We will try some Hyper-parameter tuning and see if it helps.\n",
    "\n",
    "#### First let's try tuning 'Max_Depth' of tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "67a7c9c2b842e3a0d17fba4e73be73f778c56ab8"
   },
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "val_accuracy = []\n",
    "training_f1 = []\n",
    "val_f1 = []\n",
    "tree_depths = []\n",
    "\n",
    "for depth in range(1,20):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth)\n",
    "    tree_clf.fit(X_train,y_train)\n",
    "    y_training_pred = tree_clf.predict(X_train)\n",
    "\n",
    "    training_acc = accuracy_score(y_train,y_training_pred)\n",
    "    train_f1 = f1_score(y_train,y_training_pred)\n",
    "    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n",
    "    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
    "    \n",
    "    training_accuracy.append(training_acc)\n",
    "    val_accuracy.append(val_mean_accuracy)\n",
    "    training_f1.append(train_f1)\n",
    "    val_f1.append(val_mean_f1)\n",
    "    tree_depths.append(depth)\n",
    "    \n",
    "\n",
    "Tuning_Max_depth = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Max_Depth\": tree_depths }\n",
    "Tuning_Max_depth_df = pd.DataFrame.from_dict(Tuning_Max_depth)\n",
    "\n",
    "plot_df = Tuning_Max_depth_df.melt('Max_Depth',var_name='Metrics',value_name=\"Values\")\n",
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "sns.pointplot(x=\"Max_Depth\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1feb6093458b6b30a8135902b0ea2cd6687d51ae"
   },
   "source": [
    "From above graph, we can conclude that keeping 'Max_Depth' = 3 will yield optimum Test accuracy and F1 score\n",
    "Optimum Test Accuracy ~ 0.805; Optimum F1 Score: ~0.7\n",
    "#### Visulazing Decision Tree with Max Depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2027e4690649174f65ec06eeb5e295004b28f8f8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "tree_clf = tree.DecisionTreeClassifier(max_depth = 3)\n",
    "tree_clf.fit(X_train,y_train)\n",
    "dot_data = tree.export_graphviz(tree_clf,feature_names = X.columns.tolist())\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "049d79525bed8f9c6539fa00e986fabca839c36b"
   },
   "source": [
    "From above tree, we could see that some of the leafs have less than 5 samples hence our classifier might overfit.\n",
    "We can sweep hyper-parameter 'min_samples_leaf' to further improve test accuracy by keeping max_depth to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c86f55903a45abe87fb749f82ad24cfffbe547ae"
   },
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "val_accuracy = []\n",
    "training_f1 = []\n",
    "val_f1 = []\n",
    "min_samples_leaf = []\n",
    "import numpy as np\n",
    "for samples_leaf in range(1,80,3): ### Sweeping from 1% samples to 10% samples per leaf \n",
    "    tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = samples_leaf)\n",
    "    tree_clf.fit(X_train,y_train)\n",
    "    y_training_pred = tree_clf.predict(X_train)\n",
    "\n",
    "    training_acc = accuracy_score(y_train,y_training_pred)\n",
    "    train_f1 = f1_score(y_train,y_training_pred)\n",
    "    val_mean_f1 = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='f1_macro').mean()\n",
    "    val_mean_accuracy = cross_val_score(tree_clf,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
    "    \n",
    "    training_accuracy.append(training_acc)\n",
    "    val_accuracy.append(val_mean_accuracy)\n",
    "    training_f1.append(train_f1)\n",
    "    val_f1.append(val_mean_f1)\n",
    "    min_samples_leaf.append(samples_leaf)\n",
    "    \n",
    "\n",
    "Tuning_min_samples_leaf = {\"Training Accuracy\": training_accuracy, \"Validation Accuracy\": val_accuracy, \"Training F1\": training_f1, \"Validation F1\":val_f1, \"Min_Samples_leaf\": min_samples_leaf }\n",
    "Tuning_min_samples_leaf_df = pd.DataFrame.from_dict(Tuning_min_samples_leaf)\n",
    "\n",
    "plot_df = Tuning_min_samples_leaf_df.melt('Min_Samples_leaf',var_name='Metrics',value_name=\"Values\")\n",
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "sns.pointplot(x=\"Min_Samples_leaf\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe3f6c814829be8931832a63b3a920f4623c45f7"
   },
   "source": [
    "From above plot, we will choose Min_Samples_leaf to 35 to improve test accuracy. \n",
    "\n",
    "Let's use this Decision Tree classifier on unseen test data and evaluate __Test Accuracy, F1 Score and Confusion Matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e66c2b2351d8ce910bc3faf04ad75d1abd65e15"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)\n",
    "tree_clf.fit(X_train,y_train)\n",
    "y_pred = tree_clf.predict(X_test_imp)\n",
    "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix on Test Data\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "483827c921a79654efbc26a5e5e19cd74cdb56a1"
   },
   "source": [
    "####  Mis-classifications\n",
    "It can be seen that majority of the misclassifications are happening because of Loan Reject applicants being classified as Accept.\n",
    "\n",
    "Let's look into Random Forest Classifier if it can reduce mis-classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f87b1c06ede59dceb27c45617d1b3d5b2f94a5c3"
   },
   "source": [
    "### Model 2: Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5fab14ff1b38e4b6b8f57b6f49605313bb82dbb7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "y_pred = rf_clf.predict(X_train)\n",
    "print(\"Train F1 Score \", f1_score(y_train,y_pred))\n",
    "print(\"Train Accuracy \", accuracy_score(y_train,y_pred))\n",
    "\n",
    "print(\"Validation Mean F1 Score: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='f1_macro').mean())\n",
    "print(\"Validation Mean Accuracy: \",cross_val_score(rf_clf,X_train,y_train,cv=5,scoring='accuracy').mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f6d57ca1ae24e5e562d7b14494f3c0361a2a9d8"
   },
   "source": [
    "#### Random Forest: Test Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "81ecfeffb42e244bed64392387b6145a843bbd2c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5c9802651dae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test F1 Score: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Confusion Matrix on Test Data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Predicted'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf_clf' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred = rf_clf.predict(X_test_imp)\n",
    "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix on Test Data\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff51ca5edfc4dc669cb29ff35c39bf42d788bbcd"
   },
   "source": [
    "Random Forest gives same results as Decision Tree Classifier.\n",
    "Finally, we will try Logistic Regression Model by sweeping threshold values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0865f5b0d5a38ff6a12b656e391c7ab3f7a59597"
   },
   "source": [
    "### Model 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6d26944c677c8b8123814838192deccaeb1e61e5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fcfb00e4a475>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m## Sweeping from threshold of 0.1 to 0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mlogreg_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'liblinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mlogreg_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "train_accuracies = []\n",
    "train_f1_scores = []\n",
    "test_accuracies = []\n",
    "test_f1_scores = []\n",
    "thresholds = []\n",
    "\n",
    "#for thresh in np.linspace(0.1,0.9,8): ## Sweeping from threshold of 0.1 to 0.9\n",
    "for thresh in np.arange(0.1,0.9,0.1): ## Sweeping from threshold of 0.1 to 0.9\n",
    "    logreg_clf = LogisticRegression(solver='liblinear')\n",
    "    logreg_clf.fit(X_train,y_train)\n",
    "    \n",
    "    y_pred_train_thresh = logreg_clf.predict_proba(X_train)[:,1]\n",
    "    y_pred_train = (y_pred_train_thresh > thresh).astype(int)\n",
    "\n",
    "    train_acc = accuracy_score(y_train,y_pred_train)\n",
    "    train_f1 = f1_score(y_train,y_pred_train)\n",
    "    \n",
    "    y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n",
    "    y_pred_test = (y_pred_test_thresh > thresh).astype(int) \n",
    "    \n",
    "    test_acc = accuracy_score(y_test,y_pred_test)\n",
    "    test_f1 = f1_score(y_test,y_pred_test)\n",
    "    \n",
    "    train_accuracies.append(train_acc)\n",
    "    train_f1_scores.append(train_f1)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_f1_scores.append(test_f1)\n",
    "    thresholds.append(thresh)\n",
    "    \n",
    "    \n",
    "Threshold_logreg = {\"Training Accuracy\": train_accuracies, \"Test Accuracy\": test_accuracies, \"Training F1\": train_f1_scores, \"Test F1\":test_f1_scores, \"Decision Threshold\": thresholds }\n",
    "Threshold_logreg_df = pd.DataFrame.from_dict(Threshold_logreg)\n",
    "\n",
    "plot_df = Threshold_logreg_df.melt('Decision Threshold',var_name='Metrics',value_name=\"Values\")\n",
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "sns.pointplot(x=\"Decision Threshold\", y=\"Values\",hue=\"Metrics\", data=plot_df,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2545f583fd5443bfd020381e82d8d55396e12de8"
   },
   "source": [
    "Logistic Regression does slightly better than Decision Tree and Random Forest.\n",
    "<br> Based on the above Test/Train curves, we can keep threshold to 0.4. <br>\n",
    "Now Finally let's look at Logistic Regression Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "103d4f25a79e368912cb498c82f2951cbdd3395d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-eb82ad5a250a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mthresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.4\u001b[0m \u001b[1;31m### Threshold chosen from above Curves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_test_thresh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_pred_test_thresh\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test Accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test F1 Score: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg_clf' is not defined"
     ]
    }
   ],
   "source": [
    "thresh = 0.4 ### Threshold chosen from above Curves\n",
    "y_pred_test_thresh = logreg_clf.predict_proba(X_test_imp)[:,1]\n",
    "y_pred = (y_pred_test_thresh > thresh).astype(int) \n",
    "print(\"Test Accuracy: \",accuracy_score(y_test,y_pred))\n",
    "print(\"Test F1 Score: \",f1_score(y_test,y_pred))\n",
    "print(\"Confusion Matrix on Test Data\")\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9151235fbf3e118bfbe36ac56d0f30173262ac79"
   },
   "source": [
    "Logistic Regression Confusion matrix is very similar to Decision Tree and Random Forest Classifier.\n",
    "In this analysis, we did extensive analysis of input data and were able to achieve Test Accuracy of  __86 %__\n",
    "\n",
    "__If you like this Kernel please do Upvote !!__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6be7724637fead13499c06da15cf18d5ff94b2bf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
